\chapter{Decision Rules}

When classifying the insurance risk of a vehicle we return a class on an integer of scale of -2 to 3.
We know that it describes order and that a vehicle with a high risk rating is of more risk to an insurer than a vehicle with a low risk rating.
However we do not know whether the intervals between the different risk ratings are equal.
Using a technique that recognises the ordinal nature of classes rather than treating them as nominal classes has been shown to improve the results of classification. \cite{Agresti10}.

In chapter two we discussed the need for a decision rule to allocate an object to a class.
We introduced the 0-1 loss function and chose the class that minimized expected loss.
We want to investigate alternate loss functions that makes use of the ordered class structure.
We also need new metrics to decide which loss functions perform better.

First we will introduce some common loss functions and the mean squared error to help determine how successful these loss functions are.
Then we will describe a custom approach using a loss matrix and see how we can affect the behaviour of our classifier through this.
Finally we will discuss some alternate approaches to ordered classification.

\section{Common Loss Functions}

In this section we outline some common loss functions based on those found in Berger \cite{Berger85}.

We will start with the 0-1 loss function as described in chapter two and then we will test alternate choices that take into account how close our estimate is to the true value.
We will use a uniform prior and take the posterior expectations to estimate the required probabilities as in the previous chapter.

\subsection{0-1 Loss Function}
Previously we considered the 0-1 loss function.
To recap this is defined by:
\begin{equation}
	L(c, \hat{c}) = 
	\begin{cases}
		0 & \text{if}\ c = \hat{c} \\
		1 & \text{otherwise}
	\end{cases}
\end{equation}

The expected loss is:
\begin{equation}
	E(L) = \sum_{c \in \mathcal{C}} L(c, \hat{c})P(c \mid \mathbf{a}) = 1 - P(\hat{c} \mid \mathbf{a})
\end{equation}

So to minimize our expected loss we choose:
\begin{equation}\label{map}
	\hat c = \arg\max_{c \in \mathcal{C}} P(c)\prod_{i=1}^{k}P(a_i \mid c)
\end{equation}
This is known as the maximum a posteriori (MAP) estimate.

\subsection{Squared Differences}
The squared differences loss function is defined as:
\begin{equation}
	L(c, \hat{c}) = (c - \hat{c})^2
\end{equation}
This assigns greater loss to risk ratings that are further away from the true value.

For this function the expected loss is:
\begin{equation}
	E(L) = \sum_{c \in \mathcal{C}} (c - \hat{c})^2P(c \mid \mathbf{a}) 
\end{equation}

Differentiating this with respect to $\hat{c}$ gives:
\begin{equation}
	\frac{\partial}{\partial \hat{c}} E(L) = \sum_{c \in \mathcal{C}} (-2c + 2\hat{c})P(c \mid \mathbf{a}) 
\end{equation}
Setting this equal to zero gives:
\begin{align}
	\sum_{c \in \mathcal{C}} cP(c \mid \mathbf{a}) & = \sum_{c \in \mathcal{C}} \hat{c}P(c \mid \mathbf{a}) \\
	E(c \mid \mathbf{a}) & = \hat{c}
\end{align}
So the estimate which minimizes the loss function is the expected class.
In the context of our problem the estimated class must be an integer, however this expected value may not be.

\subsection{Absolute Difference}
Finally we have the absolute difference loss function:
\begin{equation}
	L(c, \hat{c}) = | c - \hat{c} |
\end{equation}
Once again this assigns greater loss to risk ratings that are further away from the true value.

For this function the expected loss is:
\begin{align}
	E(L) & = \sum_{c \in \mathcal{C}} |c - \hat{c}|P(c \mid \mathbf{a}) \\
	     & = \sum_{c < \hat{c}} (\hat{c} - c)P(c \mid \mathbf{a}) - \sum_{c \geq \hat{c}} (\hat{c} - c)P(c \mid \mathbf{a})
\end{align}

Differentiating this with respect to $\hat{c}$ gives:
\begin{align}
	\frac{\partial}{\partial \hat{c}} E(L) & = \sum_{c < \hat{c}} P(c \mid \mathbf{a}) - \sum_{c \geq \hat{c}} P(c \mid \mathbf{a}) \\
	& = \sum_{c \leq \hat{c}} P(c \mid \mathbf{a}) - \sum_{c \geq \hat{c}} P(c \mid \mathbf{a})
\end{align}
Setting this equal to zero gives:
\begin{align}
	\sum_{c \leq \hat{c}} P(c \mid \mathbf{a}) & = \sum_{c \geq \hat{c}} P(c \mid \mathbf{a}) \\
	P(c \leq \hat{c} \mid \mathbf{a}) & = P(c \geq \hat{c} \mid \mathbf{a})
\end{align}
So the estimate that minimizes expected loss for this loss function is the median value.
This may be difficult to define on our data set.
For example suppose class -3 had $P(C = -2 \mid \mathbf{a}) = 0.5 = P(C=3 \mid \mathbf{a})$ then any class could reasonably be considered the median and therefore minimize our expected loss.
When more than one risk rating could be reasonably considered the median we will choose the one with the largest probability and then at random.

\section{Application}
We will now apply these loss functions to our automobile data set and measure accuracy and mean squared error.
Mean squared error is the average difference between the true class and assigned class squared.
We will also investigate how often the assigned classes agree.

As the expected posterior estimates perform better than the maximum likelihood estimates we shall use these to estimate the required probabilities.
We will also use the uniform hyperparameters and set $s=1$.

Using 10-fold cross validation our various loss functions perform as follows:

\begin{center}
	\begin{tabular}{l r r}
		\hline
		Loss Function                & Accuracy & MSE  \\
		\hline
		0-1                          & 69.99\%  & 0.59 \\
		Squared Difference           & -        & 0.55 \\
		Squared Difference (Rounded) & 67.88\%  & 0.58 \\
		Absolute Difference          & 69.99\%  & 0.59 \\
		\hline
	\end{tabular}
\end{center}

Note that the 0-1 loss function and the absolute difference loss function perform the same.
The squared difference loss function performs slightly worse in the accuracy metric after rounding however still has a lower MSE than the other loss functions.

The 0-1 loss function and absolute differences loss function assign classes in a very similar manner.
This is due to there often being a $\hat{c} \in \mathcal{C}$ with a much greater $P(c \mid \mathbf{a})$.
When this is the case $\hat{c}$ is the choice for both the 0-1 loss function and the absolute difference loss functions.

The squared difference loss function differs from these two slightly as it is affected more by outliers.

\section{Custom Loss Matrix}
In the previous section we considered three common choices for the loss function however there are other options available.

If we knew the true cost to the insurer of misclassifying a vehicle we could construct a specific loss function.
As an example consider the following loss matrix:
\begin{equation}
	L = 
		\begin{bmatrix}
			0   & 1  & 1  & 1  & 1  & 1 \\
			10  & 0  & 1  & 1  & 1  & 1 \\
			20  & 10 & 0  & 1  & 1  & 1 \\
			50  & 25 & 10 & 0  & 1  & 1 \\
			80  & 40 & 20 & 10 & 0  & 1 \\
			100 & 50 & 30 & 20 & 10 & 0 \\
		\end{bmatrix}
\end{equation}
Using this matrix we can define a loss function as $L(c, \hat{c}) = L_{c,\hat{c}}$.
In this example we assign increasing loss for underestimating the risk rating of a vehicle as this would cause the insurer to offer lower premiums despite a possibly high chance of payout.
We also assign a consistent loss of one for overestimating the risk to avoid always assigning the maximum risk rating.
This should cause our decision rule to underestimate the risk rating.
The expected loss would then be calculated as:
\begin{equation}
	E(L) = \sum_{c \in \mathcal{C}} L_{c,\hat{c}}P(c \mid \mathbf{a})
\end{equation}
The estimate that minimizes this expectation can then be chosen providing our decision rule.

If we use this particular loss function in our naive Bayes classifier and compare it to the zero one loss function we get:
\begin{center}
	\begin{tabular}{l r r}
		\hline
		Loss Function & Accuracy & MSE  \\
		\hline
		0-1           & 69.43\%  & 0.59 \\
		Custom        & 65.28\%  & 0.70 \\
		\hline
	\end{tabular}
\end{center}

Note that this custom loss function has a lower accuracy and a greater mean squared error.
However if we compare the two confusions matrices:

Zero-One:
\begin{center}
\begin{tabular}{l l r r r r r r}
    \hline
                       &    & \multicolumn{6}{c}{Predicted Class}                   \\
    \hline
                       &    & -2      & -1    & 0       & 1       & 2       & 3     \\
    \hline
\multirow{6}{*}{Actual Class} & -2 & 0.52\% & 1.04\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
& -1 & 1.55\% & 5.7\% & 3.63\% & 0.52\% & 0.0\% & 0.0\% \\
& 0 & 0.0\% & 1.55\% & 24.87\% & 4.66\% & 1.55\% & 0.0\% \\
& 1 & 0.0\% & 2.07\% & 2.59\% & 17.62\% & 3.11\% & 1.04\% \\
& 2 & 0.0\% & 0.0\% & 1.04\% & 2.07\% & 11.92\% & 1.04\% \\
& 3 & 0.0\% & 0.52\% & 0.0\% & 0.52\% & 2.07\% & 8.81\% \\
\hline
\end{tabular}
\end{center}

Custom Loss Matrix:
\begin{center}
\begin{tabular}{l l r r r r r r}
    \hline
                       &    & \multicolumn{6}{c}{Predicted Class}                   \\
    \hline
                       &    & -2      & -1    & 0       & 1       & 2       & 3     \\
    \hline
\multirow{6}{*}{Actual Class} & -2 & 0.0\% & 1.55\% & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
& -1 & 0.52\% & 5.7\% & 4.66\% & 0.52\% & 0.0\% & 0.0\% \\
& 0 & 0.0\% & 1.04\% & 21.76\% & 5.7\% & 4.15\% & 0.0\% \\
& 1 & 0.0\% & 1.55\% & 1.55\% & 17.1\% & 4.66\% & 1.55\% \\
& 2 & 0.0\% & 0.0\% & 1.04\% & 2.07\% & 11.92\% & 1.04\% \\
& 3 & 0.0\% & 0.52\% & 0.0\% & 0.52\% & 2.07\% & 8.81\% \\
\hline
\end{tabular}
\end{center}

We see that our custom loss function is much less likely to underestimate the risk rating of a vehicle.
This may be more beneficial to an insurer than a more accurate classifier that sometimes underestimates the risk.

\section{Conclusions}

% Implicitly restate your thesis/position.
We have found that using an alternative loss function to create the decision rule can lead to increases in some metrics.
We saw that using the squared differences loss function reduces the mean squared error but at the expense of accuracy.
We also saw how a custom choice of loss matrix can affect our classifications.

% Emphasize the importance of your subject by placing it in a larger context.
Ordered classification falls between ordinary classification and regression and has not been fully explored despite it being a common occurrence in the real world.
We have demonstrated that varying the loss function allows us to achieve different goals.

% Offer suggestions for the future based on what you have argued.
Further work on identifying how to deal with ordered classes is required given their prominence.
An alternate approach to ordinal classification was described by Frank and Hall \cite{Frank01}.
They proposed reducing the classification to a series of binary classifications.
For our insurance problem this would involved deciding whether the vehicle's true risk rating is greater than each possible risk rating.
Then after making each of these judgements we are able to assign a class.
They showed this method can lead to a more accurate classifier than one that ignores ordering when handling ordinal classes.