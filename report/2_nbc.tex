\chapter{Naive Bayes Classifier}

As previously stated, we want a classifier that will learn from a set of vehicles and their risk ratings and, given a new observation, be able to return the risk rating of this new vehicle.

In this chapter we will use Manning's \cite{Manning08} formulation of the naive Bayes classifier and apply it to our insurance problem.
We will not be introducing any ideas that have not already been discussed thoroughly in literature.

We will outline the theory behind the naive Bayes classifier and identify points for further investigation.
We will then introduce a method of measuring how successful our classifier is before applying it to our problem.
We will show that the naive Bayes classifier performs poorly at this particular problem without any modifications.

\section{Previous Applications}
A Bayesian approach to artificial intelligence has been used since the 1960s, especially in a medical setting \cite{Russell03}.
The naive Bayes classifier has been shown to be highly effective in many different situations and comparable in performance to more complex classifiers \cite{Ashari13}.

In one particular study \cite{Dumitru09} images of breast mass for patients who had previously suffered from breast cancer were analysed.
The naive Bayes classifier used attributes of the cells in the image such as radius and symmetry to determine whether the cancer was recurrent or non-recurrent.
In this study the classifier achieved an accuracy of 74.24\% which is consistent with the best results of other classifiers.
Accuracy is the proportion of the objects for which the classifier allocated the true class.
However this same study indicates there is room for improvement.
The sensitivity of the classifier was only 27.78\%.
Sensitivity is the proportion of true positives meaning in this study a large proportion of sick patients were not correctly diagnosed.

\section{Theory}

To help explain how the naive Bayes classifier works we will introduce a new data set from a remote sensing study.
This study aimed to identify the type of forest based on satellite readings \cite{Forrest}.
The study measured spectral information in the green, red and infrared wavelengths on three separate dates of $523$ different areas of forest in Japan.
In total each observation has nine continuous attributes and falls in to one of four possible classes: Sugi forest, Hinoki forest, Mixed deciduous forest and other non-forest land.
This data set was chosen as it contained a large number of observations and demonstrates a situation where the naive Bayes classifier works well.

Formally, let us denote the class variable by $C$, taking values from the set $\mathcal{C} = \{0,1,2,3\}$ for each of the four types of forest.
We measure 9 features $A_1,\dots,A_9$ which we discretize so that they all take values from the set $\mathcal{A} = \{0,\dots,9\}$.
The observations of the variables are denoted by $c$ and $a_1,\dots,a_9$ respectively.

We are interested in the probability of a forest being of type $c$ given sensor readings $\mathbf{a}$ i.e. $P(c \mid \mathbf{a})$.
Using Bayes theorem we can rewrite this as:
\begin{equation} \label{bayes}
	P(c \mid \mathbf{a}) = \frac{P(c)P(\mathbf{a} \mid c)}{P(\mathbf{a})}
\end{equation}
Bayes theorem was first described in Thomas Bayes' posthumous work ``An Essay towards solving a Problem in the Doctrine of Chances'' \cite{Bayes63}.

Next we introduce the naivety assumption.
The naivety assumption states that the attributes of an object are conditionally independent given the class.
When the NBC was originally used for information retrieval this assumption referred to the terms in a document appearing independently of one another in the document's index \cite{Maron60}.
In the context of this data set we are assuming the sensor readings are conditionally independent given the type of forest.
This assumption may not necessarily be accurate however it simplifies the problem by making the probabilities easier to estimate.
We can therefore write the probability of observing an object with attributes $a_1, \dots\ a_9$ given it is in class $c$ as:
\begin{equation} \label{naivety}
	P(\mathbf{a} \mid c) = \prod_{i=1}^9 P(a_i \mid c)
\end{equation}

Note that $P(\mathbf{a})$ is independent of $c$ and is just a scaling constant.
So by bringing together \cref{bayes,naivety} we can write:
\begin{equation}
	P(c \mid \mathbf{a}) \propto P(c)\prod_{i=1}^{9}P(a_i \mid c)
\end{equation}

To turn this into a classifier we need a decision rule to decide which class a forest falls into based on these probabilities.
We can introduce a loss function to allow us to choose the class.
The standard choice for the naive Bayes classifier is the 0-1 loss function \cite{Rish01} defined as:
\begin{equation}\label{0-1_loss_function}
	L(c, \hat{c}) = 
	\begin{cases}
		0 & \text{if}\ c = \hat{c} \\
		1 & \text{otherwise}
	\end{cases}
\end{equation}
This function assigns a loss of 1 to any wrong classification regardless of the class that is assigned. 
Under this loss function we can write the expected loss as:
\begin{equation}
	E(L) = \sum L(c, \hat{c})P(c \mid \mathbf{a}) = 1 - P(\hat{c} \mid \mathbf{a})
\end{equation}
So to minimize our expected loss we choose:
\begin{equation} \label{map_estimate}
	\hat c = \arg\max_{c \in \mathcal{C}} P(c)\prod_{i=1}^{k}P(a_i \mid c)
\end{equation}
This is known as the maximum a posteriori (MAP) estimate.
Note that when using this loss function our estimates for the probabilities do not necessarily have to be accurate, they just have to be ordered correctly.
We will investigate other options for the loss function later on in this report.

Now that we have our method for making our decision we need to estimate the required probabilities.

To do so we parametrise these probabilities.
We denote the unknown chances of observing an object in class $c$ by $\theta_c$ and the chance of observing an object in class $c$ with attributes $\mathbf{a}$ by $\theta_{\mathbf{a}, c}$.
Similarly we denote the conditional chances of observing an individual attribute $a_i$ and a set of attributes $\mathbf{a}$ given $C=c$ by $\theta_{a_i \mid c}$ and $\theta_{\mathbf{a} \mid c}$ respectively.

Now we have parametrised the probabilities that we wish to estimate, we can consider the likelihood function for $\mathbf{\theta}$, the vector whose elements are the chances $\theta_{c}$ and $\theta_{a_i \mid c}$.
Using our data we denote the frequencies of objects in each class $c$ by $n(c)$ and the number of objects in class $c$ with attribute $a_i$ by $n(a_i, c)$.
For example, returning to the forest type data set, the number of observations of class $0$ is $158$ so $n(0) = 158$.
Note that $\sum_{c \in \mathcal{C}}n(c) = N$ and $\sum_{a_i \in \mathcal{A}_i}n(a_i, c) = n(c)$.
We then consider the vector $\mathbf{n}$ which contains these frequencies.

We can derive the likelihood function for $\mathbf{\theta}$ given the frequencies described above:
\begin{align} \label{likelihood}
	l(\mathbf{\theta} \mid \mathbf{n}) & =  \prod_{c \in \mathcal{C}} \prod_{\mathbf{a} \in \mathbf{\mathcal{A}}} \theta_{\mathbf{a}, c}^{n(\mathbf{a}, c)} \\
	& = \prod_{c \in \mathcal{C}} \prod_{\mathbf{a} \in \mathbf{\mathcal{A}}} \left[ \theta_{c}^{n(\mathbf{a}, c)} \theta_{\mathbf{a} \mid c}^{n(\mathbf{a}, c)} \right] \\
	& = \prod_{c \in \mathcal{C}} \left[ \theta_{c}^{\sum_{\mathbf{a} \in \mathbf{\mathcal{A}}} n(\mathbf{a}, c)} \prod_{\mathbf{a} \in \mathbf{\mathcal{A}}} \prod_{i=1}^k \theta_{a_i \mid c}^{n(\mathbf{a}, c)} \right] \\
	& \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{n(c)} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{n(a_i, c)} \right]
\end{align}

Now we want to estimate these probabilities.
A simple estimate for these parameters is the maximum likelihood estimate (MLE).
To find the MLE first we take the log likelihood:
\begin{equation}
	L(\mathbf{\theta} \mid \mathbf{n}) \propto \sum_{c \in \mathcal{C}}  n(c)log(\theta_c) + \sum_{c \in \mathcal{C}} \sum_{i=1}^k \sum_{a_i \in \mathcal{A}_i} n(a_i, c) log(\theta_{a_i \mid c}) 
\end{equation}
We want to maximise the likelihood function.
To do so we maximise each part of the log likelihood function which we will do so through the use of Lagrange multipliers.
This is a strategy for finding local maxima and minima of a function subject to constraints.

For the $\theta_c$ parameters we want to maximise:
\begin{equation}
	f(\mathbf{\theta}, \mathbf{n}) = \sum_{c \in \mathcal{C}}  n(c)log(\theta_c)
\end{equation}
under the constraint:
\begin{equation}\label{theta_c constraint}
	g(\mathbf{\theta}, \mathbf{n}) = \sum_{c \in \mathcal{C}}  \theta_c - 1 = 0
\end{equation}
This gives us our Lagrangian:
\begin{equation}
	\mathcal{L}(\mathbf{\theta}, \mathbf{n}, \lambda) = \sum_{c \in \mathcal{C}}  n(c)log(\theta_c) - \lambda(\sum_{c \in \mathcal{C}}  \theta_c - 1)
\end{equation}

Differentiating with respect to $\theta_c$ we have:
\begin{equation}
	\nabla_{\theta_c} \mathcal{L}(\mathbf{\theta}, \mathbf{n}, \lambda) = \frac{n(c)}{\theta_c} - \lambda
\end{equation}

Setting this to zero and using the constraint in \cref{theta_c constraint} we have maximum likelihood estimate:
\begin{equation}
	\hat\theta_c = \frac{n(c)}{N}
\end{equation}
This is just the relative frequency of observations that fall into that class.
Returning to our example data set we have $N=523$ and $n(0)=158$ so $\hat\theta_0 = \frac{158}{523} \approx 0.302$
% something else here

Similarly for the $\theta_{a_i \mid c}$ parameters we want to maximize:
\begin{equation}
	f(\mathbf{\theta}, \mathbf{n}) = \sum_{c_i \in \mathcal{A}_i} n(a_i, c) log(\theta_{a_i \mid c})
\end{equation}
for each $c \in \mathcal{C}$ and $i \in \{1,\dots,k\}$ under the constraint:
\begin{equation}
	g(\mathbf{\theta}, \mathbf{n}) = \sum_{a_i \in \mathcal{A}_i}  \theta_{a_i \mid c} - 1 = 0
\end{equation}
We again solve this using the Lagrangian to find the maximum likelihood estimate:
\begin{equation}
	\hat\theta_{a_i \mid c} = \frac{n(a_i, c)}{n(c)}
\end{equation}

We can now estimate the required probabilities and hence have our naive Bayes classifier.
First we estimate the probabilities by taking the maximum likelihood estimate for their parametrisation:
\begin{align}
	P(c) & = \frac{n(c)}{N} \\
	P(a_i \mid c) & = \frac{n(a_i, c)}{n(c)}
\end{align}
Then for an object with an unknown class we choose the MAP estimate given in \cref{map_estimate}.

\section{Diagnostics}

To measure how successful our classifier is we will initially use a technique known as $k$-fold cross validation.
In $k$-fold cross validation we split our dataset into $k$ equally sized groups.
Then each group is individually excluded, the classifier is trained on all the remaining groups then tested on the excluded group.
We then average all these measurements to return an estimate for the measurement of our classifier.

The choice of $k$ leads to different types of cross validation.
A special case of cross validation is when $k=n$ (the number of observations).
This is knowns as leave-one-out cross validation \cite{Priddy05}.

Kohavi studyed the effect of different values for $k$ on $k$-fold cross validation \cite{Kohavi95}.
He concluded that small values of $k$, in the study 2 and 5, lead to a large amount of bias and an underestimate of the accuracy.
However for $k=10$ the estimate is almost unbiased.
He also noted a high level of variance for both small values of $k$ and large values of $k$ when carrying out leave one out cross validation.

This is an instance of bias-variance tradeoff \cite{James13}.
We will use a moderate value for k, $k=10$, throughout this report to keep both variance and bias low.

Initially we will consider two metrics.
The first is accuracy; this is the percentage of correctly classified objects restricted to the case where there is a single MAP estimate.
The second is indeterminate classifications; this is percentage of objects with no unique MAP estimate for their class and usually occurs when $P(c \mid \mathbf{a}) = 0$ for all $c \in \mathcal{C}$.

The accuracy of the classifier is 81.77\% and the percentage of indeterminate classifications is 2.04\% on the forest data set, using 10-fold cross validation.

Previous work carried out by Johnson et al. \cite{Johnson12} on the data set aimed to improve their classification by weighting training data based on geographical distance.
They used SVMs (Support Vector Machines) to classify the forests and recorded an accuracy of 85.9\%.
This indicates that our relatively simple NBC can still achieve a high degree of accuracy and be used effectively for classification.

\section{Application to Insurance Problem}

Now we have seen an application of the NBC to a straightforward data set we want to apply it to our insurance problem.

Unlike in the trees data set, in the automobile data set we have objects with missing values for attributes.
We have no mechanism for considering these so we must discard these observations for now.
This is known as Listwise Deletion.
This reduces our data set from 205 observations to 193 observations.

To simplify this data set we will discretize the continuous variables.
There are methods to model these variables for example using normal distributions \cite{Dumitru09} however, for simplicity, we shall not consider these in this report.
We split the continuous variables into 10 categories with equal frequency.

As before we carry out 10-fold cross validation for the two metrics.
The accuracy of our classifier is 63.73\% on the automobile data set.
This is considerably worse than the example forest data set.
Moreover our classifier was indeterminate about 18.13\% of the objects.

Clearly the classifier performs better on the forest type dataset than on the automobile data set.
One reason for this is the larger sample size for the forest type data set.

However our classifier primarily falls down if there are no observations with attribute $a_j$ and class $c$ in our training set.
In these case the maximum likelihood estimate for $\theta_{a_j \mid c}$ is $0$.
This estimate leads to $P(c \mid \mathbf{a}) = 0$ and would rule out assigning any objects with the attribute $a_j$ to class $c$.
This is especially problematic for small sets of data and can explain the large percentage of indeterminate classifications we saw when applying the classifier to the automobile insurance data set.

\section{Conclusions and Improvements}

% Implicitly restate your thesis/position.
In this chapter we examined a simple classifier which allowed us to start to solve the problem of determining automobile insurance risk.
We explained the theory behind the NBC as presented by Manning and applied it to a simpler remote sensing problem as well as our insurance problem.

% Emphasize the importance of your subject by placing it in a larger context.
We saw that the naive Bayes classifier performed well on the simpler remote sensing problem and achieved a comparable accuracy to a more complex classifier.
The success of the naive Bayes classifier is well known and other studies have shown it to be comparable with other classifiers \cite{Dumitru09}.

However we also saw that the NBC was not very accurate when applied to our insurance problem.
We saw that it was often indeterminate about classifications.
This is a common problem and has been discussed in literature \cite{Need}.

% Offer suggestions for the future based on what you have argued.
In the next chapter we will tackle the problem of indeterminate classifications by introducing a prior distribution for our $\mathbf{\theta}$ chances.