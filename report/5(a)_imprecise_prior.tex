\newcommand{\sn}[2]{\ensuremath{{#1}\times 10^{#2}}}

\chapter{Imprecise Prior}

When estimating the probabilities for our classifier we take into account our prior beliefs for them and the likelihood given a set of observations.
When choosing our prior distribution we had to pick hyperparameters without any knowledge.
In this chapter we will use Walley's imprecise Dirichlet model \cite{Walley96} to model this lack of knowledge and then create a simple interval based credal classifier.

\section{Imprecise Dirichlet Model}

When we initially chose our prior distribution we chose the hyperparameters in \cref{initial prior} such that they follow the principle of indifference.
However as previously mentioned this prior does not truly represent a lack of prior knowledge.
We need a way to represent this lack of knowledge and acknowledge that our estimates for the probabilities depend on our choice of hyperparameters.

The imprecise Dirichlet model is a model for this lack of knowledge introduced by Walley \cite{Walley96}.
Instead of using a single prior distribution to represent our beliefs about the unknown parameters we use a set of prior distributions.

In the imprecise Dirichlet model the prior distributions are Dirichlet distributions with parameters $(s, \mathbf{t})$ such that $\sum_{c \in \mathcal{C}} t(c) = 1$.
The distributions for the likelihood are multinomial so:
\begin{equation}
	f(\mathbf{n} \mid \bm{\theta}) \propto \prod_{c \in \mathcal{C}} \theta_c^{n(c)}
	\qquad
	f(\bm{\theta} \mid s, \mathbf{t}) \propto \prod_{c \in \mathcal{C}} \theta_c^{st(c) - 1}
\end{equation}
Then the posterior distribution is of the form:
\begin{equation} \label{dirichlet_pdf2}
	f(\mathbf{\theta} \mid \mathbf{n}, s, \mathbf{t}) \propto \prod_{c \in \mathcal{C}} \theta_c^{n(c) + st(c) - 1}
\end{equation}
which is also a Dirichlet distribution with parameters $(N+s, \frac{\mathbf{n}+s\mathbf{t}}{N+s})$.
We can then take the posterior expectation for the $\bm{\theta}$ chances giving:
\begin{equation}
	P_t(c) = E(\theta_c \mid \mathbf{n}, s, \mathbf{t}) = \frac{n(c)+st(c)}{N+s}
\end{equation}
Note this is a function of $t(c)$ which allow us to obtain upper and lower estimates for for probabilities by varying $t(c)$:
\begin{align}
	\overline{P}(c) & = \frac{n(c)+s}{N+s} & (t(c) \rightarrow 1) \\
	\underline{P}(c) & = \frac{n(c)}{N+s}  & (t(c) \rightarrow 0)
\end{align}

Zaffalon applied this method of imprecise priors to the problem of classification.
We will use a set of the prior distributions of the form in \cref{prior} for a fixed value of $s$.
The parameter $s$ represents the strength of our prior beliefs and determines how quickly our classifier learns.

\section{Imprecise Probabilities}

Imprecise probability refers to the partial specification of a probability for example through upper and lower bounds for a probability.
Imprecise probabilities have other applications in artificial intelligence and can better represent an experts knowledge \cite{Coolen11}.
Using the model of imprecise priors we can find the upper and lower bounds for each posterior expectation based on different prior distributions.

Recall that we estimate the probabilities by taking the expectation of the posterior distribution and that these estimates depend on $\mathbf{t}$ i.e.:
\begin{align}
	P_t(c) & = E(\theta_c \mid \mathbf{n},s,\mathbf{t}) = \frac{n(c) + st(c)}{N + s} \\
	P_t(a_i \mid c) & = E(\theta_{a_i \mid c} \mid \mathbf{n},s,\mathbf{t}) = \frac{n(a_i, c) + st(a_i, c)}{n(c) + st(c)}
\end{align}
We can then use these to find upper and lower estimates for the probabilities over all values of $\mathbf{t}$ in our prior model.
For our distributions the upper and lower bounds are given by:
\begin{align}
	\overline{P}(c) & = \frac{n(c) + s}{N+s} \\
	\underline{P}(c) & = \frac{n(c)}{N+s}
\end{align}
These occur when we use the prior distributions with $t(c) \rightarrow 0$ and $t(c) \rightarrow 1$ respectively.

Similarly we have:
\begin{align}
	\overline{P}(a_i \mid c) & = \frac{n(a_i, c) + s}{n(c)+s} \\
	\underline{P}(a_i \mid c) & = \frac{n(a_i, c)}{n(c)+s}
\end{align}
These occur when we use the prior distributions with $t(c) \rightarrow 1$, $t(a_i, c)\rightarrow1$ and $t(c) \rightarrow 1$, $t(a_i, c)\rightarrow0$ respectively.

Let's start by comparing how our classifier behaves if we assume the true probability is at each end of the interval.
We will use the 0-1 loss function for the decision method and estimate each probability $P(\cdot)$ by either the upper or lower probability of our interval.
We will measure accuracy and indeterminate classifications as before.

\begin{center}
	\begin{tabular}{l|c c}
	                & Accuracy & Indeterminate Assignments \\
	\hline
	Lower Estimates & 62.17\%  & 20.02\%            \\
	Upper Estimates & 40.09\%  & 0\%                \\
	\end{tabular}
\end{center}

Neither of these offer a sufficient classification.
There are often no observations of an attribute with a particular class which is why using the lower estimates leads to indeterminate classifications despite a reasonable accuracy for this data set.
On the other hand when using the upper estimates our classifier has very low accuracy.

\section{Simple Credal Classifier}
Alternatively we can turn our classifier into a credal classifier.
A credal classifier assigns a set of classes as opposed to a single class to an object.

Recall the MAP estimate for the risk rating of a vehicle given by \cref{map}.
We say that a class $c'$ is dominated by $c''$ if:
\begin{equation}\label{Credal Dominance}
	P_t(c' \mid \mathbf{a}) < P_t(c'' \mid \mathbf{a})
\end{equation}
for all values of $\mathbf{t}$ in our prior model.
This is because the MAP estimate for the class will always choose $c''$ over $c'$ regardless of which prior is used.
Note that this is equivalent to $P_t(c', \mathbf{a}) < P_t(c'', \mathbf{a})$ as $P(\mathbf{a})$ does not depend on $c$.

We can use the bounds for the probabilities we found earlier to create an interval $P(c, \mathbf{a})$ must lie in. 

If we look at the intervals created by the upper and lower estimates we achieve:
\begin{equation}
	P_t(c, \mathbf{a}) \in \left[ \underline{P}(c)\prod_{i=1}^k \underline{P}(a_i \mid c), \overline{P}(c)\prod_{i=1}^k \overline{P}(a_i \mid c) \right]
\end{equation}
for each $c \in \mathcal{C}$.
This is true because:
\begin{equation}
\underline{P}(c)\prod_{i=1}^k \underline{P}(a_i \mid c) \leq \underline{P}(c, \mathbf{a}) \leq P_t(c, \mathbf{a}) \leq \overline{P}(c, \mathbf{a}) \leq \overline{P}(c)\prod_{i=1}^k \overline{P}(a_i \mid c)
\end{equation}
for all choices of $\mathbf{t}$.
We can use the above intervals to create a simple credal classifier.

For an example consider the following intervals:
\begin{center}
	\begin{tabular}{l|c c}
	Risk Rating & Lower Bound & Upper Bound \\
	\hline
	-2          & $0$              & $\sn{3.12}{-9}$  \\
	-1          & $0$              & $\sn{8.91}{-16}$ \\
	0           & $\sn{3.81}{-15}$ & $\sn{2.30}{-13}$ \\
	1           & $\sn{2.19}{-9}$  & $\sn{2.34}{-8}$  \\
	2           & $\sn{1.88}{-13}$ & $\sn{6.22}{-11}$ \\
	3           & $0$              & $\sn{5.82}{-17}$ \\
	\end{tabular}
\end{center}
We can see the classes -1, 0, 2 and 3 are dominated by 1.
However the risk rating of -2 is not dominated by 1 as $\sn{3.12}{-9} > \sn{2.19}{-9}$.
Hence our credal classifier returns the set of risk ratings $\{-2, 1\}$.
Note that in this particular example the true risk rating was 1 so our credal classifer was correct to include it in the set of possible classes.

\section{Diagnostics}

We now need a way to test this classifier.
We cannot use our previous measure of accuracy as this classifier may not return a single class.
Instead there are a few metrics we can use for our diagnostics \cite{Antonucci11}:
\begin{description}
	\item[Single Accuracy (A\%)] Accuracy of the credal classifier when a single class is returned
	\item[Set Accuracy (B\%)] Percentage of objects for which the true class is in the returned set when the set has size larger than one
	\item[Indeterminate Output Size (C)] Average set size for returned sets containing more than one class
	\item[Determinacy (D\%)] Percentage of objects for which the returned set contains one class
\end{description}

\section{Application}

We will vary the choice of the hyperparameter $s$ when measuring these three statistics to see its effect.

%Seed = 0.1

\begin{center}
\begin{tabular}{l|c c c c}
        & A\%     & B\%     & C    & D\%     \\
\hline
s = 0.5 & 77.02\% & 87.05\% & 3.71 & 38.34\% \\
s = 1   & 76.92\% & 90.67\% & 3.75 & 20.20\% \\
s = 2   & 75.00\% & 94.30\% & 4.04 & 3.11\% \\
s = 5   & -       & 97.92\% & 5.22 & 0\%   \\
\end{tabular}
\end{center}

We see that the single accuracy of our classifier slightly decreases for the different $s$ parameters.
We also note that this single accuracy is greater than the accuracy of our corrected naive Bayes classifier on the same data set.
However we notice that varying the $s$ parameter has an effect on the other two metrics.
Increasing the value of $s$ decreases determinacy and increases the set accuracy.

This effect can be easily explained.
Increasing the $s$ value increases the upper bound and decreases the lower bound on each of the probabilities being estimated.
Hence increasing the value of $s$ increases the size of the interval and increasing the size of the interval leads to less intervals being dominated and fewer classes being excluded.

\section{Conclusion}

In this chapter we have seen how Walley's imprecise Dirichlet model can be used to create intervals for the probabilities we wish to estimate when using the naive Bayes classifier.
We have seen that using either the upper and lower bounds lead to a very poor classifier.
We then used these imprecise probabilities to create a simple credal classifier.