\chapter{Naive Bayes Classifier}

\section{History}

\section{Theory}

Formally, let us denote the class variable by $C$, taking values in the set $\mathcal{C}$. Also we measure $k$ features $A_1,\dots,A_k$ from the sets $\mathcal{A}_1,\dots,\mathcal{A}_k$. We denote observations of these variables as $c$ and $a_1,\dots,a_k$ respectively.

We are interested in $P(c \mid \mathbf{a})$. Using Bayes theorem we can rewrite this as:
\begin{equation}
	P(c \mid \mathbf{a}) = \frac{P(\mathbf{a} \mid c)P(c)}{P(\mathbf{a})}
\end{equation}

Moreover we can make use of the naivety assumption. The naivety assumptions states that each attribute is independent of one another. We can now write the probability of an object being in class $c$ with attributes $a_1,\dots,a_k$ as:
\begin{equation}
	P(c \mid \mathbf{a}) = \frac{P(c)\prod_{i=1}^{k}P(a_i \mid c)}{P(\mathbf{a})}
\end{equation}

To turn this into a classifier we need a way to make a decision for which class an object falls into based on the estimated probabilities. A common method is choosing the class that maximises $P(c \mid \mathbf{a})$. This is known as the maximum a posteriori (MAP) estimate. We also note that $P(\mathbf{a})$ is not dependent on $C$ hence we can write our MAP estimate as:
\begin{equation}
	c_{MAP} = argmax_{c \in \mathcal{C}} P(c)\prod_{i=1}^{k}P(a_i \mid c)
\end{equation}

Now that we have our method for making our decision we need to estimate the required probabilities.

Firstly we parametrise these probabilities.We denote the unknown chances of observing an object with $C=c$ by $\theta_c$ and the change of observing an object with $C=c$ and $\mathbf{A} = \mathbf{a}$ by $\theta_{\mathbf{a}, c}$. Similarly we denote the conditional chances of $A_i=a_i$ and $(A_1,\dots,A_k)=(a_1,\dots,a_k)$ given $C=c$ by $\theta_{a_i \mid c}$ and $\theta_{\mathbf{a} \mid c}$ respectively.

We can consider the likelihood function for the $\mathbf{\theta}$, the vector whose elements are the chances $\theta_{\mathbf{a}, c}$ given data $\mathbf{n}$, the vector of all known frequencies.

The likelihood function can be expressed as:
\begin{equation} \label{likelihood}
	l(\mathbf{\theta} \mid \mathbf{n}) \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{n(c)} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{n(a_i, c)} \right]
\end{equation}

A simple estimate for these parameters is the maximum likelihood estimate (MLE). To find the MLE first we take the log likelihood:
\begin{equation}
	L(\mathbf{\theta} \mid \mathbf{n}) \propto \sum_{c \in \mathcal{C}}  n(c)log(\theta_c) + \sum_{c \in \mathcal{C}} \sum_{i=1}^k \sum_{a_i \in \mathcal{A}_i} n(a_i, c) log(\theta_{a_i \mid c}) 
\end{equation}
So to maximise the likelihood function we need to maximise the two parts of the log likelihood function.

To do so we use the method of Lagrange multipliers. This is a strategy for finding local maxima and minima of a function subject to constraints.

For the first equation we have
\begin{align}
	f(\mathbf{\theta}, \mathbf{n})& = \sum_{c \in \mathcal{C}}  n(c)log(\theta_c) \\
	g(\mathbf{\theta}, \mathbf{n})& = \sum_{c \in \mathcal{C}}  \theta_c - 1
\end{align}
This gives us our Lagrangian:
\begin{equation}
	\mathcal{L}(\mathbf{\theta}, \mathbf{n}, \lambda) = \sum_{c \in \mathcal{C}}  n(c)log(\theta_c) - \lambda(\sum_{c \in \mathcal{C}}  \theta_c - 1)
\end{equation}

Differentiating with respect to $\theta_c$ we have:
\begin{equation}
	\nabla_{\theta_c} \mathcal{L}(\mathbf{\theta}, \mathbf{n}, \lambda) = \frac{n(c)}{\theta_c} - \lambda
\end{equation}

Hence the maximum is achieved giving an mle of $\hat{\theta_c} = \frac{n(c)}{N}$. Intuitively this is just the relative frequency of observations that fall into that class.

We now have our naive Bayes classifier. We estimate $P(c)$ by $\frac{n(c)}{N}$ and $P(a_i \mid c)$ by $\frac{n(a_i, c)}{n(c)}$, the relative frequencies. Then we choose the class $c$ which maximises $P(c)\prod_{i=1}^{k}P(a_i \mid c)$.

\section{Diagnostics}

We use a technique known as $k$-fold cross validation to evaluate accuracy. In $k$-fold cross validation we split our dataset into $k$ equally sized groups. Then for each group we train the classifier on all the other groups and test it on that group. We then average all these accuracy to return an (unbiased?) estimate for the accuracy of our classifier.

The choice of $k$ leads to different types of cross validation. A standard choice is $k=10$. A special case of cross validation is when $k=n$ (the number of observations). This is knowns as \textit{Leave-one-out cross validation} \cite{Priddy05}.

\section{Applications}

\subsection{Forest Type Data Set}

First we will apply this classifier to a data set from a remote sensing study. The study measured spectral information in the green, red and infrared wavelengths on three separate dates of different forest types in Japan. In total we have nine continuous attributes and four possible classes: Sugi forest, Hinoki forest, Mixed deciduous forest and other non-forest land.

To make our data appropriate for this method we discrete the continuous variables into $n$ bins with an equal frequency.

Using cross validation to determine the accuracy of this classifier has 80\% accuracy on this data set.

\subsection{Automobile Data Set}

To make our data appropriate for this method we discrete the continuous variables into $n$ bins with an equal frequency. We also discard any objects with missing values giving us a total of 200 objects to test out classifier on.

Using cross validation to determine the accuracy of this classifier has 66\% accuracy on this data set.

\section{Conclusions}

Clearly the classifier performs better on the forest type dataset than on the auto mobile data set. This could be due to a few different reasons.

Firstly our classifier falls down if there are no observations with attribute $a_k$ and class $c$ in our training set. In these case the maximum likelihood estimate for $\theta_{a_k \mid c}$ is 0. This estimate leads to $P(c \mid \mathbf{a}) = 0$ and would rule out assigning any objects with the attribute $a_k$ to class $c$. This is especially problematic for small sets of data. We can tackle this by introducing prior probabilities for the theta chances.

Another reason the classifier appears to perform worse on our auto mobile data set could be the nature of its categories. The accuracy metric does not take into account how close the classification is. For example if the true class is 2 an assigned class of 1 should be considered better than an assigned class of -2.

