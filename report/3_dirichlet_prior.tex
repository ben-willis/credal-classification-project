\chapter{Corrected NBC}

\section{Theory}

We return to our likelihood function \cref{likelihood} for our theta variables.
We can introduce a prior distribution for these parameters and then consider the posterior distribution.

The Dirichlet distribution is the multinomial extension of the beta distribution for $\theta_1,\dots,\theta_k$ where $\theta_i \in (0,1)$ and $\sum_{i=1}^k \theta_i = 1$ with probability density function:
\begin{equation} \label{dirichlet_pdf}
	f(\theta_1,\dots,\theta_k \mid s, t(1),\dots,t(k)) \propto \prod_{i=1}^k \theta_i^{st(i) - 1}
\end{equation}
where $s > 0$ and each $t(i)>0$ such that $\sum_{i=1}^{k}t(i) = 1$.

We introduce a distribution that is similar to our likelihood as our prior density:
\begin{equation} \label{prior}
	f(\mathbf{\theta} \mid \mathbf{t}, s) \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{st(c) - 1} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{st(c, a_i) - 1} \right]
\end{equation}
This is in the same form as the likelihood however each $n(\cdot)$ is replaces by $st(\cdot) - 1$.
$s > 0$ is a fixed constant and we have the following constraints on $t(\cdot):$
\begin{align}\label{prior_constraints}
	\sum_{c \in \mathcal{C}} t(c) & = 1 \\
	\sum_{a_i \in \mathcal{A}_i} t(a_i, c) & = t(c) && \forall i, c \\
	t(a_i, c) & > 0 && \forall i, a_i, c
\end{align}
These constraints retain the structure seen for the frequencies in our likelihood.

This distribution is the conjugate prior for the likelihood function \cref{likelihood}.
When we multiply our likelihood by this prior density we get a posterior in the same family as this prior.
If the prior has hyper parameters $st(\cdot)$ the posterior will have hyper parameters $st(\cdot) + n(\cdot)$.

We can now estimate the parameters by taking the posterior expectation:
\begin{align}
	E(\theta_c \mid \mathbf{n},s,\mathbf{t}) & = \frac{n(c) + st(c)}{N + s} = \hat{\theta}_c \\
	E(\theta_{a_i \mid c} \mid \mathbf{n},s,\mathbf{t}) & = \frac{n(a_i, c) + st(a_i, c)}{n(c) + st(c)} = \hat{\theta}_{a_i \mid c}
\end{align}

\section{Application}

To apply this classifier to our data set we need to choose the hyper parameters of the prior distribution.
We note that $s$ affects the speed at which our classifier learns and $t(\cdot)$ represents our beliefs for the probabilities.
We want our prior to be as indifferent as possible \cite{laplace1812} to represent out lack of information.
We want assign the same probabilities to each chance so we set:
\begin{align}\label{initial prior}
	t(c) & = \frac{1}{|C|} \\
	t(a_i, c) & = \frac{1}{|A_i||C|}
\end{align}
Following Walley's recommendation \cite{Walley96}, we also set $s=1$.

Previously we've considered the accuracy of our classifier.
An alternative metric is the mean squared error of our estimate.
\begin{equation}
	\text{MSE} = \frac{1}{n}\sum_{i=1}^n(\hat{c_i} - c_i)^2
\end{equation}
This indicates how close to the true class our classifications are.

The accuracy, mean squared error and percentage of random classifications are shown below:
\begin{center}
	\begin{tabular}{ c|c c c c c c }
		              & Accuracy & MSE   & Random Classifications\\
		\hline
		NBC           & 59.95\%  & 2.823 & 22.45\% \\
		Corrected NBC & 68.17\%  & 0.642 & 0\%
	\end{tabular}
\end{center}

\section{Conclusions}
Using posterior expectations instead of maximum likelihood estimates improves our classifier in all three metrics.

Firstly introducing the prior distribution means that we no longer have random classifications as $P(c \mid \mathbf{a})$ will never be zero.
We will investigate how sensitive our choice of classifications to changes in the prior hyper parameters.

A proportion vehicles that were previously randomly classified are now classified correctly.
This has led to an increase in the accuracy of our classifier.
However we are still achieving an accuracy of less than 70\% which leaves great room for improvement.

Additionally the mean square error has decreased dramatically.
I'm not really sure why this is.
Note that the loss function we are using to make our decision after estimating the probabilities does not take in to account how close our prediction is to the actual risk.
An alternate loss function may reduce the MSE even further.