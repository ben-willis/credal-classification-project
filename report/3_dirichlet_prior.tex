\chapter{Corrected NBC with Dirichlet Prior}

\section{Theory}

We return to our likelihood function \cref{likelihood} for our theta variables.
We can introduce a prior distribution for these parameters and then consider the posterior distribution.

The Dirichlet distribution is the multinomial extension of the beta distribution for $\theta_1,\dots,\theta_k$ where $\theta_i \in (0,1)$ and $\sum_{i=1}^k \theta_i = 1$ with probability density function:
\begin{equation} \label{dirichlet_pdf}
	f(\theta_1,\dots,\theta_k \mid s, t(1),\dots,t(k)) \propto \prod_{i=1}^k \theta_i^{st(i) - 1}
\end{equation}
where $s > 0$ and each $t(i)>0$ such that $\sum_{i=1}^{k}t(i) = 1$.

We introduce a distribution that is similar to our likelihood as our prior density:
\begin{equation} \label{prior}
	f(\mathbf{\theta} \mid \mathbf{t}, s) \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{st(c) - 1} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{st(c, a_i) - 1} \right]
\end{equation}
This is in the same form as the likelihood however each $n(\cdot)$ is replaces by $st(\cdot) - 1$.
$s > 0$ is a fixed constant and we have the following constraints on $t(\cdot):$
\begin{align}\label{prior_constraints}
	\sum_{c \in \mathcal{C}} t(c) & = 1 \\
	\sum_{a_i \in \mathcal{A}_i} t(a_i, c) & = t(c) && \forall i, c \\
	t(a_i, c) & > 0 && \forall i, a_i, c
\end{align}

This distribution is the conjugate prior for the likelihood function \cref{likelihood}.
When we multiply our likelihood by this prior density we get a posterior in the same family as this prior.
If the prior has hyperparameters $st(\cdot)$ the posterior will have hyperparameters $st(\dot) + n(\cdot)$.

We can now estimate the parameters by taking the MAP estimates i.e.
\begin{equation}
	\hat{\theta_c} = \frac{n(c) + st(c)}{N + s}
\end{equation}

We estimate:
\begin{align}
	P(c) & \text{ by } \frac{n(c) + st(c)}{N + s} \\
	P(a_i \mid c) & \text{ by } \frac{n(a_i, c) + st(a_i, c)}{n(c) + st(c)}
\end{align}

\section{Application}{}

To apply this classifier to our data set we need to choose the hyperparameters of the prior distribution.
We note that $s$ affects the speed at which our classifier learns and $t(\cdot)$ represents our beliefs for $\theta_\cdot$.
To comply with the constraints \ref{prior_constraints} let us set:
\begin{align}
	s & = 1 \\
	t(c) & = \frac{1}{|C|} \\
	t(a_i, c) & = \frac{1}{|A_i||C|}
\end{align}

New accuracy of classifier on auto mobile data set is $64.75\%$.

An alternative metric is the mean squared error of our estimate.
\begin{equation}
	\text{MSE} = \frac{1}{n}\sum_{i=1}^n(\hat{c_i} = c_i)^2
\end{equation}

The naive Bayes classifier in the first chapter on the auto mobile data set had a MSE of $3.66$.
This new classifier with the prior distribution has a MSE of $1.05$.
This again shows an improvement over the previous classifier.


Can we consider a different metric other than success? As our classes are integer classes we could consider the squared error of our classification.

\section{Conclusions}