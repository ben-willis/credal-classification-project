\chapter{Corrected NBC}

In the previous section we considered a frequentist approach to estimating the probabilities required for the NBC.
We saw that our classifier falls down if there are no observations with attribute $a_j$ and class $c$ in our training set.
If we take a Bayesian approach to the problem and introduce a prior distribution for these probabilities we can avoid this situation.
In literature this is often called ``additive smoothing'' or ``Laplace smoothing'' and is presented as simply adding a pseudocount to each of the observed frequencies \cite{Manning08}.
This is often done without properly considering the prior distribution that is being used.
In this chapter we will offer a rigorous definition of our prior distribution which will form the basis for the naive Credal classifier we discuss later.

\section{Introducing a Prior}

We return to our likelihood function \cref{likelihood} for our theta variables.
We will introduce a prior distribution for these parameters and then consider the posterior distribution.

The Dirichlet distribution is the multinomial extension of the beta distribution for $\theta_1,\dots,\theta_k$ where $\theta_i \in (0,1)$ and $\sum_{i=1}^k \theta_i = 1$ with probability density function:
\begin{equation} \label{dirichlet_pdf}
	f(\theta_1,\dots,\theta_k \mid s, t(1),\dots,t(k)) \propto \prod_{i=1}^k \theta_i^{st(i) - 1}
\end{equation}
where $s > 0$ and each $t(i)>0$ such that $\sum_{i=1}^{k}t(i) = 1$.
The family of Dirichlet distributions is a conjugate prior for the multinomial distribution.

We introduce a distribution that has similar properties to the Dirichlet distribution and has a similar structure to our likelihood as our prior density:
\begin{equation} \label{prior}
	f(\mathbf{\theta} \mid \mathbf{t}, s) \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{st(c) - 1} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{st(c, a_i) - 1} \right]
\end{equation}
This is in the same form as the likelihood however each $n(\cdot)$ is replaces by $st(\cdot) - 1$.
$s > 0$ is a fixed constant and we have the following constraints on $t(\cdot):$
\begin{align}\label{prior_constraints}
	\sum_{c \in \mathcal{C}} t(c) & = 1 \\
	\sum_{a_i \in \mathcal{A}_i} t(a_i, c) & = t(c) && \forall i, c \\
	t(a_i, c) & > 0 && \forall i, a_i, c
\end{align}
These constraints retain the structure seen for the frequencies in our likelihood.

This family of distributions is a conjugate prior for the likelihood function \cref{likelihood}.
When we multiply our likelihood by this prior density we get a posterior in the same family as this prior.
If the prior has hyper parameters $st(\cdot)$ the posterior will have hyper parameters $st(\cdot) + n(\cdot)$.

We can now estimate the parameters by taking the posterior expectation:
\begin{align}
	E(\theta_c \mid \mathbf{n},s,\mathbf{t}) & = \frac{n(c) + st(c)}{N + s} = \hat{\theta}_c \\
	E(\theta_{a_i \mid c} \mid \mathbf{n},s,\mathbf{t}) & = \frac{n(a_i, c) + st(a_i, c)}{n(c) + st(c)} = \hat{\theta}_{a_i \mid c}
\end{align}

\section{Hyper Parameters}

To apply this classifier to our data set we need to choose the hyper parameters of the prior distribution.

We note that $s$ represents the strength of our prior distribution when calculating the posterior and hence determines how quickly our classifier learns from the observed data.
As $s \rightarrow 0$ we simply get the maximum likelihood estimates.
On the other hand if $s$ is extremely large the observed frequencies are dominated by our prior beliefs.
Walley recommends setting $s=1$ or $s=2$ \cite{Walley96}.
When comparing the NBC to the corrected NBC we will set $s=1$ then we will look at the sensitivity of the corrected NBC with respect to the $s$ value.

The other hyperparameters $t(\cdot)$ represent our prior beliefs for $P(\cdot)$.
Initially we have no information regarding what these probabilities are and Laplace tells us \cite{laplace1812} the best way to represent this in through indifference.
This is referred to as ``The Principal of Indifference'' by Keynes \cite{Keynes21} who criticised it as a representation of a lack of prior information.
He presented several situations where this principal leads to paradoxes.
In this section we will follow the principal of indifference when selecting our prior distribution however we will return to this problem in chapter 4 where we introduce the imprecise prior.
Hence we set:
\begin{align}\label{initial prior}
	t(c)      & = \frac{1}{|C|} \\
	t(a_i, c) & = \frac{1}{|A_i||C|}
\end{align}
These assignments obey the constraints given in \cref{prior_constraints}.


\section{Application}

The accuracy and percentage of indeterminate classifications are shown below:
\begin{center}
	\begin{tabular}{ c|c c }
		              & Accuracy & Indeterminate Classifications\\
		\hline
		NBC           & 63.73\%  & 20.21\% \\
		Corrected NBC & 72.02\%  & 0\%
	\end{tabular}
\end{center}

In addition we can consider the confusion matrix:
\begin{center}
    \begin{tabular}{c c c c c c c c}
    \hline
                       &    & \multicolumn{6}{c}{Predicted Class}                   \\
    \hline
                       &    & -2      & 1       & 0       & 1       & 2       & 3   \\
    \hline
    \multirow{4}{*}{Actual Class} & -2 & \textbf{0.0\%}  & 1.55\% & 0.0\%   & 0.0\%   & 0.0\%  & 0.0\%  \\
                       & -1 & 1.55\% & \textbf{5.7\%}  & 3.63\%  & 0.52\%  & 0.0\%  & 0.0\%  \\
                       & 0  & 0.0\%  & 1.55\% & \textbf{24.87\%} & 4.66\%  & 1.55\% & 0.0\%  \\
                       & 1  & 0.0\%  & 2.59\% & 2.07\%  & \textbf{19.17\%} & 1.04\% & 1.55\% \\
                       & 2  & 0.0\%  & 0.0\%  & 1.55\%  & 2.07\%  & \textbf{11.4\%} & 1.04\% \\
                       & 3  & 0.0\%  & 0.0\%  & 0.52\%  & 0.52\%  & 2.07\% & \textbf{8.81\%} \\
    \hline
    \end{tabular}
\end{center}

Using posterior expectations instead of maximum likelihood estimates improves our classifier in all three metrics.

Firstly introducing the prior distribution means that we no longer have indeterminate classifications as $P(c \mid \mathbf{a})$ will never be zero.
We will investigate how sensitive our choice of classifications to changes in the prior hyper parameters.

A proportion of vehicles that previously had indeterminate classifications are now classified correctly.
This has led to an increase in the accuracy of our classifier.
However we are still achieving an accuracy of less than 70\% which leaves great room for improvement.

We now look at the sensitivity of the accuracy of the corrected NBC to changes in the $s$ hyper-parameter.
If we choose $s$ values log uniformly between $10^{-4}$ and $10^{6}$ and plot these against the accuracy of the corrected NBC using these s values we get the following plot:
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    title={Accuracy},
    xlabel={$s$},
    ylabel={\%},
   	xmode = log,
    ymin=40, ymax=80,
    ymajorgrids=true,
    xmajorgrids=true,
    grid style=dashed,
]

\addplot[
	mark=o,
    color=black,
    ]
    coordinates {
    (0.001,74.09)
    (0.01,74.61)
    (0.1,73.58)
    (1,72.02)
    (10,70.47)
    (100,67.36)
    (1000,49.22)
    (10000,44.04)
    (100000, 44.56)
    };
 
\end{axis}
\end{tikzpicture}
\end{center}

The most obvious feature is the significant drop in accuracy between $s=100$ and $s=1000$.
This is where the prior dominates the likelihood and the posterior expectations for the probabilities become uniform.
For $s>1000$ our classifier assigns classes at random uniformly.

On the other side of the graph, for $s \leq 1$ we see a fairly consistent classifier with accuracy greater than 70\%.

We've seen that the corrected NBC has a greater accuracy than the normal NBC and never returns indeterminate classifications.
We've also seen the effect varying the hyperparameter $s$ has on this classifier.
In the next section we'll look at our choice of loss function in the original formulation of the NBC.