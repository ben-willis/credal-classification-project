\documentclass{beamer}
 
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
 
\title{Credal Classification of Auto Mobile Insurance Risk}
\author{Ben Willis}
\institute{Durham University}
\date{2017}
 
\begin{document}
 
\frame{\titlepage}

\begin{frame}
	\frametitle{Problem}
	\begin{center}
		\begin{tabular}{l c c c c|c}
			Make       & Length & $\dots$ & Horsepower & Price & Risk \\
			\hline
			Volvo      & 188.8  & $\dots$ & 114        & 12940 & -2   \\
			Audi       & 192.7  & $\dots$ & 110        & 18920 & 2    \\
			Mitsubishi & 172.4  & $\dots$ & 116        & 9279  & 1    \\
			Audi       & 176.6  & $\dots$ & 115        & 17450 & ?
		\end{tabular}
	\end{center}
	\begin{itemize}
		\item Problem of quantifying the risk to an insurer of an auto mobile.
		\item Data set contains technical information about 205 vehicles and an expert's assessment of their risk.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Naive Bayes Classifier}
		\begin{block}{Probabilistic Interpretation}
			Bayes theorem and the naivety assumption gives us:
			\begin{equation}
				P(c \mid \mathbf{a}) \propto P(c)\prod_{i=1}^{k}P(a_i \mid c)
			\end{equation}
		\end{block}
		\begin{block}{Choosing the Risk}
			\begin{equation}
				\hat c = \arg\max_{c \in \mathcal{C}} P(c)\prod_{i=1}^{k}P(a_i \mid c)
			\end{equation}
			This is known as the maximum a posteriori (MAP) estimate.
		\end{block}
\end{frame}

\begin{frame}
	\frametitle{Estimating Chances}
	To estimate these probabilities we parametrise them.
	\begin{block}{The Likelihood Function}
		\begin{equation}\label{likelihood function}
			l(\mathbf{\theta} \mid \mathbf{n}) \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{n(c)} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{n(a_i, c)} \right]
		\end{equation}
		\begin{description}
			\item[$\theta_\cdot$] Parameter for unknown $P(\cdot)$
			\item[$n(\cdot)$] Number of times $\cdot$ observed
		\end{description}
	\end{block}
	\begin{block}{Maximum Likelihood Estimates}
		\begin{equation}
			\hat{\theta}_c = \frac{n(c)}{N} \qquad \hat{\theta}_{a_i \mid c} = \frac{n(a_i, c)}{n(c)}
		\end{equation}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Application and Results}
	\begin{itemize}
		\item Discretize continuous variables
		\item Discard objects with missing values
		\item \textit{k-fold cross validation} to estimate accuracy
		\item Our classifier has  an accuracy of 59.95\% on the auto mobile data set.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Introducing the Prior}
	\begin{block}{Prior Distribution}
		We introduce a conjugate prior for the likelihood function eq. (\ref{likelihood function}):
		\begin{equation}
			f(\mathbf{\theta} \mid \mathbf{t}, s) \propto \prod_{c \in \mathcal{C}} \left[ \theta_c^{st(c) - 1} \prod_{i=1}^k \prod_{a_i \in \mathcal{A}_i} \theta_{a_i \mid c}^{st(c, a_i) - 1} \right]
		\end{equation}
		with hyper parameters $s>0$ and $\mathbf{t}$ such that:
		\begin{equation}
			\sum_{c \in \mathcal{C}} t(c) = 1, \sum_{a_i \in \mathcal{A}_i} t(a_i \mid c) = t(c) \text{ and } t(a_i \mid c) > 0
		\end{equation}
	\end{block}
	To comply with the constraints prior constraints let us set:
	\begin{equation}
		s = 1, t(c) = \frac{1}{|C|} \text{ and } t(a_i, c) = \frac{1}{|A_i||C|}
	\end{equation}
\end{frame}

\begin{frame}
	\frametitle{Diagnostics}
	Introduce the mean squared error
	\begin{tabular}{ c|c c c c c c }
		              & Accuracy & MSE   & Failed Classifications \\
		\hline
		NBC           & 59.95\%  & 2.823 & 22.45\%                \\
		Corrected NBC & 68.17\%  & 0.689 & 0\%
	\end{tabular}
\end{frame}

\begin{frame}
	\frametitle{Credal Classifier}
	Not sure what I want to put here. Something explaining the theory behind how this can be extended to create a credal classifier (Sets of priors, credal dominance).
\end{frame}

\begin{frame}
	\frametitle{Future Work}
	Summary of findings so far and future work
\end{frame}
 
\end{document}